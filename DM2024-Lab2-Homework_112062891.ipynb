{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:Pheiroijam Prishika\n",
    "\n",
    "Student ID:112062891\n",
    "\n",
    "GitHub ID:Ph-Prishika\n",
    "\n",
    "Kaggle name: prishika_matu\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "\n",
    " ![Screenshot from 2024-12-05 00-40-46.png](attachment:Screenshot from 2024-12-05 00-40-46.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "mport pandas as pd\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "tweets = pd.read_json('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', lines=True)\n",
    "labels = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "data_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset\n",
    "print(\"Tweets Dataset:\")\n",
    "print(tweets.head())\n",
    "print(\"\\nLabels Dataset:\")\n",
    "print(labels.head())\n",
    "print(\"\\nData Identification Dataset:\")\n",
    "print(data_identification.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_details(tweets_df):\n",
    "    # Extract nested fields\n",
    "    tweets_df['tweet_id'] = tweets_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "    tweets_df['text'] = tweets_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "    tweets_df['hashtags'] = tweets_df['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "    return tweets_df.drop(columns=['_source', '_index', '_type', '_crawldate', '_score'])\n",
    "\n",
    "def merge_data(tweets, labels, split_info):\n",
    "    # Merge tweets with labels and split information\n",
    "    merged = tweets.merge(labels, on='tweet_id', how='left')\n",
    "    merged = merged.merge(split_info, on='tweet_id', how='left')\n",
    "    return merged\n",
    "\n",
    "def main():\n",
    "    # Load datasets\n",
    "    tweets, labels, split_info = load_data()\n",
    "    tweets = extract_tweet_details(tweets)\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged_data = merge_data(tweets, labels, split_info)\n",
    "    \n",
    "    # Display the first 10 entries of the merged dataframe in a styled table\n",
    "    print(\"First 10 entries of the merged data:\")\n",
    "    styled_merged_data = merged_data.head(10).style.set_properties(**{\n",
    "        'border': '1px solid black',\n",
    "        'text-align': 'left',\n",
    "        'background-color': '#f9f9f9',\n",
    "        'padding': '5px'\n",
    "    }).set_caption(\"Merged Dataset: First 10 Entries\").set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#f4f4f4'), ('font-weight', 'bold')]}\n",
    "    ])\n",
    "    display(styled_merged_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_with_stemming_and_lemmatization(text):\n",
    "    # Remove emojis, special symbols, and non-alphanumeric characters\n",
    "    text = re.sub(\"[\"\n",
    "                  u\"\\U0001F600-\\U0001F64F\"\n",
    "                  u\"\\U0001F300-\\U0001F5FF\"\n",
    "                  u\"\\U0001F680-\\U0001F6FF\"\n",
    "                  u\"\\U0001F700-\\U0001F77F\"\n",
    "                  u\"\\U0001F780-\\U0001F7FF\"\n",
    "                  u\"\\U0001F800-\\U0001F8FF\"\n",
    "                  u\"\\U0001F900-\\U0001F9FF\"\n",
    "                  u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                  u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                  u\"\\U00002702-\\U000027B0\"\n",
    "                  u\"\\U000024C2-\\U0001F251\"\n",
    "                  \"]+\", '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Apply stemming and lemmatization\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n",
    "        processed_words.append(lemmatized_word)\n",
    "    \n",
    "    # Join the processed words back into a single string\n",
    "    processed_text = ' '.join(processed_words)\n",
    "    return processed_text\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_and_preprocess_data():\n",
    "    # Load datasets\n",
    "    tweets = pd.read_json('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', lines=True)\n",
    "    tweets['_source'] = tweets['_source'].apply(lambda x: x['tweet'])\n",
    "    tweets = pd.concat([tweets.drop('_source', axis=1), pd.json_normalize(tweets['_source'])], axis=1)\n",
    "    labels = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "    data_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n",
    "    \n",
    "    # Merge datasets\n",
    "    tweets = tweets.merge(labels, on='tweet_id', how='left')\n",
    "    tweets = tweets.merge(data_identification, on='tweet_id', how='left')\n",
    "    \n",
    "    # Clean and process text\n",
    "    tweets['text'] = tweets['text'].apply(clean_text_with_stemming_and_lemmatization)\n",
    "    return tweets\n",
    "\n",
    "# Load and preprocess data\n",
    "tweets = load_and_preprocess_data()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data = tweets[tweets['identification'] == 'train']\n",
    "test_data = tweets[tweets['identification'] == 'test']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['_type', 'hashtags', '_crawldate']\n",
    "train_data = train_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "test_data = test_data.drop(columns=columns_to_drop + ['emotion'], errors='ignore')\n",
    "\n",
    "\n",
    "# Save and display datasets\n",
    "def save_and_display_datasets(merged_data):\n",
    "    # Separate into train and test datasets\n",
    "    train_data = merged_data[merged_data['identification'] == 'train']\n",
    "    test_data = merged_data[merged_data['identification'] == 'test']\n",
    "    \n",
    "    # Save to CSV\n",
    "    train_data.to_csv('/kaggle/working/train_dataset.csv', index=False)\n",
    "    test_data.to_csv('/kaggle/working/test_dataset.csv', index=False)\n",
    "\n",
    "    print(\"Datasets saved as train_dataset.csv and test_dataset.csv\")\n",
    "\n",
    "    # Display first 10 rows of train and test datasets as styled tables\n",
    "    print(\"Train Dataset (First 10 Entries):\")\n",
    "    display(train_data.head(10).style.set_caption(\"Train Dataset\").set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#d9edf7'), ('font-weight', 'bold')]}]))\n",
    "    \n",
    "    print(\"Test Dataset (First 10 Entries):\")\n",
    "    display(test_data.head(10).style.set_caption(\"Test Dataset\").set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#f2dede'), ('font-weight', 'bold')]}]))\n",
    "\n",
    "def main():\n",
    "    # Load datasets\n",
    "    tweets, labels, split_info = load_data()\n",
    "    \n",
    "    # Extract tweet details and remove emojis\n",
    "    tweets = extract_tweet_details(tweets)\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged_data = merge_data(tweets, labels, split_info)\n",
    "    \n",
    "    # Save and display datasets\n",
    "    save_and_display_datasets(merged_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets if they are not already in memory\n",
    "train_data = pd.read_csv('/kaggle/working/train_data.csv')\n",
    "test_data = pd.read_csv('/kaggle/working/test_data.csv')\n",
    "\n",
    "# Remove the '_score' column from both datasets\n",
    "train_data = train_data.drop(columns=['_score','_index'], errors='ignore')\n",
    "test_data = test_data.drop(columns=['_score','_index'], errors='ignore')\n",
    "\n",
    "# Save the modified datasets back to CSV files\n",
    "train_data.to_csv('/kaggle/working/train_data.csv', index=False)\n",
    "test_data.to_csv('/kaggle/working/test_data.csv', index=False)\n",
    "\n",
    "# Print a few rows to confirm the column is removed\n",
    "print(\"Updated Training Data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nUpdated Testing Data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming train_data and test_data are already loaded and available\n",
    "# If not, load or regenerate them as per previous steps\n",
    "\n",
    "# Save the training data\n",
    "train_data.to_csv('/kaggle/working/train_data.csv', index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# Save the testing data (assuming 'emotion' column has been removed)\n",
    "test_data.to_csv('/kaggle/working/test_data.csv', index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the training and testing datasets\n",
    "train_data = pd.read_csv('/kaggle/working/train_data.csv')\n",
    "test_data = pd.read_csv('/kaggle/working/test_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_data['emotion'] = label_encoder.fit_transform(train_data['emotion'])\n",
    "\n",
    "print(\"Training Data Sample:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nTest Data Sample:\")\n",
    "print(test_data.head())\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, labels=True):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        if self.labels:\n",
    "            item['label'] = torch.tensor(self.data.iloc[idx]['emotion'], dtype=torch.long)\n",
    "        return item\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(train_data['emotion'].unique())\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TweetDataset(train_data, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TweetDataset(test_data, tokenizer, MAX_LEN, labels=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "model.save_pretrained('bert_model')\n",
    "tokenizer.save_pretrained('bert_model')\n",
    "print(\"Predicting...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "tweet_ids = test_data['tweet_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predictions to original emotion labels\n",
    "predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "# Create the submission file\n",
    "submission = pd.DataFrame({'id': tweet_ids, 'emotion': predicted_labels})\n",
    "submission.to_csv('Submission.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
